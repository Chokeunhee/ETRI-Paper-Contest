{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "S1-S4 Column Prediction model \"MultiOutputClassifier\" 전용 전처리 코드입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import glob\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "코드 실행 전에 다음 파일들이 `./data` 경로에 포함되어야 합니다.\n",
    "\n",
    "```\n",
    ".\n",
    "├── [4.0K]  answer_sample.csv\n",
    "├── [9.5K]  README_2020.txt\n",
    "├── [4.0K]  test\n",
    "│   ├── [627M]  ch2024_test__m_acc_part_5.parquet.gzip\n",
    "│   ├── [1.1G]  ch2024_test__m_acc_part_6.parquet.gzip\n",
    "│   ├── [1.0G]  ch2024_test__m_acc_part_7.parquet.gzip\n",
    "│   ├── [935M]  ch2024_test__m_acc_part_8.parquet.gzip\n",
    "│   ├── [912K]  ch2024_test__m_activity.parquet.gzip\n",
    "│   ├── [3.2M]  ch2024_test__m_ambience.parquet.gzip\n",
    "│   ├── [ 12M]  ch2024_test__m_gps.parquet.gzip\n",
    "│   ├── [ 93K]  ch2024_test__m_light.parquet.gzip\n",
    "│   ├── [203K]  ch2024_test__m_usage_stats.parquet.gzip\n",
    "│   ├── [935K]  ch2024_test__w_heart_rate.parquet.gzip\n",
    "│   ├── [106K]  ch2024_test__w_light.parquet.gzip\n",
    "│   └── [909K]  ch2024_test__w_pedo.parquet.gzip\n",
    "├── [4.0K]  train\n",
    "│   ├── [4.0K]  user01\n",
    "│   ├── [4.0K]  user02\n",
    "│   ├── [4.0K]  user03\n",
    "│   ├── [4.0K]  user04\n",
    "│   ├── [4.0K]  user05\n",
    "│   ├── [4.0K]  user06\n",
    "│   ├── [4.0K]  user07\n",
    "│   ├── [4.0K]  user08\n",
    "│   ├── [4.0K]  user09\n",
    "│   ├── [4.0K]  user10\n",
    "│   ├── [4.0K]  user11\n",
    "│   ├── [4.0K]  user12\n",
    "│   ├── [4.0K]  user21\n",
    "│   ├── [4.0K]  user22\n",
    "│   ├── [4.0K]  user23\n",
    "│   ├── [4.0K]  user24\n",
    "│   ├── [4.0K]  user25\n",
    "│   ├── [4.0K]  user26\n",
    "│   ├── [4.0K]  user27\n",
    "│   ├── [4.0K]  user28\n",
    "│   ├── [4.0K]  user29\n",
    "│   ├── [4.0K]  user30\n",
    "│   ├── [1.1K]  user_info_2020.csv\n",
    "│   ├── [ 73K]  user_sleep_2020.csv\n",
    "│   └── [ 89K]  user_survey_2020.csv\n",
    "├── [ 18K]  train_label.csv\n",
    "├── [4.0K]  val\n",
    "│   ├── [1.3G]  ch2024_val__m_acc_part_1.parquet.gzip\n",
    "│   ├── [563M]  ch2024_val__m_acc_part_2.parquet.gzip\n",
    "│   ├── [662M]  ch2024_val__m_acc_part_3.parquet.gzip\n",
    "│   ├── [838M]  ch2024_val__m_acc_part_4.parquet.gzip\n",
    "│   ├── [853K]  ch2024_val__m_activity.parquet.gzip\n",
    "│   ├── [3.7M]  ch2024_val__m_ambience.parquet.gzip\n",
    "│   ├── [ 15M]  ch2024_val__m_gps.parquet.gzip\n",
    "│   ├── [ 88K]  ch2024_val__m_light.parquet.gzip\n",
    "│   ├── [192K]  ch2024_val__m_usage_stats.parquet.gzip\n",
    "│   ├── [925K]  ch2024_val__w_heart_rate.parquet.gzip\n",
    "│   ├── [101K]  ch2024_val__w_light.parquet.gzip\n",
    "│   └── [901K]  ch2024_val__w_pedo.parquet.gzip\n",
    "└── [2.9K]  val_label.csv\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 경로 설정\n",
    "data_dir = \"../data\"  # 데이터 저장 경로\n",
    "preprocessed_dir_moc = \"../data_preprocessed_moc\"  # 전처리 완료된 데이터를 저장할 경로"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user time series concat\n",
    "for user_folder in filter(os.path.isdir, sorted(glob.glob(os.path.join(data_dir, \"train\", \"user*\")))):\n",
    "    user_id = os.path.basename(user_folder)\n",
    "    \n",
    "    for session_folder in tqdm(sorted(glob.glob(os.path.join(user_folder, \"*\")))):\n",
    "        session_id = os.path.basename(session_folder)\n",
    "        \n",
    "        file_name = f\"{user_id}_{datetime.datetime.fromtimestamp(int(session_id), datetime.timezone(datetime.timedelta(hours=9))).strftime('%Y%m%d')}.csv\"\n",
    "        if os.path.join(preprocessed_dir_moc, \"train\", file_name) in glob.glob(os.path.join(preprocessed_dir_moc, \"train\", \"*\")):\n",
    "            print(f\"Skipping {file_name} because it already exists\")\n",
    "            continue\n",
    "        session_df = None\n",
    "        for sensor_folder in filter(os.path.isdir, sorted(glob.glob(os.path.join(session_folder, \"*\")))):\n",
    "            sensor_type = os.path.basename(sensor_folder)\n",
    "\n",
    "            df = None\n",
    "\n",
    "            for timestamp_csv in sorted(glob.glob(os.path.join(sensor_folder, \"*.csv\"))):\n",
    "                tmp = pd.read_csv(timestamp_csv)\n",
    "                # timestamp 열에 파일 이름 숫자를 더하기\n",
    "                tmp[\"timestamp\"] = tmp[\"timestamp\"].apply(lambda x: x + float(os.path.basename(timestamp_csv).split(\".\")[0]))\n",
    "                tmp[\"time\"] = tmp[\"timestamp\"].apply(lambda x: datetime.datetime.fromtimestamp(x, datetime.timezone(datetime.timedelta(hours=9))))\n",
    "                if df is None:\n",
    "                    df = tmp\n",
    "                else:\n",
    "                    df = pd.concat([df, tmp])\n",
    "            if df is None:\n",
    "                continue\n",
    "            columns_order = ['time'] + [col for col in df.columns if col != 'time' and col != 'timestamp']\n",
    "            df = df[columns_order]\n",
    "            df.columns = [sensor_type + \"_\" + col if col != 'time' else col for col in df.columns]\n",
    "            df.set_index(\"time\", inplace=True)\n",
    "\n",
    "            grouped = df.resample('1min')\n",
    "\n",
    "            mean_df = grouped.mean()\n",
    "            var_df = grouped.var()\n",
    "\n",
    "            result_df = mean_df.join(var_df, lsuffix='_mean', rsuffix='_var')\n",
    "        \n",
    "            if session_df is None:\n",
    "                session_df = result_df\n",
    "            else:\n",
    "                session_df = pd.concat([session_df, result_df], axis=1)\n",
    "        # save file\n",
    "        os.makedirs(os.path.join(preprocessed_dir_moc, \"train\", \"before_label\"), exist_ok=True)\n",
    "\n",
    "        new_filename = f\"{user_id}_{result_df.index[0].strftime('%Y%m%d')}.csv\"\n",
    "        session_df.to_csv(os.path.join(preprocessed_dir_moc, \"train\", \"before_label\", new_filename))\n",
    "    print(f\"{user_id} preprocess done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simplify_column_name(column_name):\n",
    "    parts = column_name.split('_')\n",
    "    if len(parts) >= 3 and parts[1] == parts[2]:\n",
    "        return '_'.join([parts[0], parts[1], parts[3]])\n",
    "    return column_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for user_folder in filter(os.path.isdir, sorted(glob.glob(os.path.join(data_dir, \"train\", \"user*\")))):\n",
    "    user_id = os.path.basename(user_folder)\n",
    "    \n",
    "    for session_folder in tqdm(sorted(glob.glob(os.path.join(user_folder, \"*\")))):\n",
    "        session_id = os.path.basename(session_folder)\n",
    "        session_date = datetime.datetime.fromtimestamp(int(session_id), datetime.timezone(datetime.timedelta(hours=9))).strftime('%Y%m%d')\n",
    "\n",
    "        train_df = pd.read_csv(os.path.join(preprocessed_dir_moc, \"train\", \"before_label\", f\"{user_id}_{session_date}.csv\"), index_col=0)\n",
    "        train_df.index = pd.to_datetime(train_df.index)\n",
    "\n",
    "        train_action_df = pd.read_csv(os.path.join(session_folder, f\"{session_id}_label.csv\"))\n",
    "        train_action_df.index = train_action_df[\"ts\"].apply(lambda x: datetime.datetime.fromtimestamp(x, datetime.timezone(datetime.timedelta(hours=9))))\n",
    "        train_action_df.index = pd.to_datetime(train_action_df.index)\n",
    "        \n",
    " \n",
    "        merged = pd.merge(train_df, train_action_df, how='outer', left_index=True, right_index=True)\n",
    "\n",
    "        os.makedirs(os.path.join(preprocessed_dir_moc, \"train\", \"after_label\"), exist_ok=True)\n",
    "        merged.to_csv(os.path.join(preprocessed_dir_moc, \"train\", \"after_label\", f\"{user_id}_{session_date}.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for files in sorted(glob.glob(os.path.join(preprocessed_dir_moc, \"train\", \"after_label\", \"user*\"))):\n",
    "    df = pd.read_csv(files, index_col=0)\n",
    "    tst = [\"s_\" + col[2:].lower() if col.startswith(\"e4\")\n",
    "           else \"m_\" + col[1:].lower() if col.startswith(\"m\")\n",
    "           else col for col in df.columns]\n",
    "    tst = [simplify_column_name(col) for col in tst]\n",
    "    df.rename(columns=dict(zip(df.columns, tst)), errors='ignore', inplace=True)\n",
    "    df.to_csv(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_train_data(user_id, train_directory):\n",
    "    train_csv = [f for f in os.listdir(train_directory) if f.startswith(f'user{user_id:02d}') and f.endswith('.csv')]\n",
    "    train_user = pd.DataFrame()\n",
    "    \n",
    "    for file in train_csv:\n",
    "        train_df = pd.read_csv(os.path.join(train_directory, file))\n",
    "        train_user = train_df if train_df.empty else pd.concat([train_user, train_df], ignore_index=True)\n",
    "    # print(train_user.columns)\n",
    "    train_user['timestamp'] = pd.to_datetime(train_user[\"time\"])\n",
    "    train_user['date'] = train_user['timestamp'].dt.date\n",
    "    \n",
    "    daily_summary = train_user.groupby('date').agg({\n",
    "        'm_acc_x_mean': 'mean',\n",
    "        'm_acc_x_var': 'var',\n",
    "        'm_acc_y_mean': 'mean',\n",
    "        'm_acc_y_var': 'var',\n",
    "        'm_acc_z_mean': 'mean',\n",
    "        'm_acc_z_var': 'var',\n",
    "        'activity': 'mean',\n",
    "        's_hr_mean': 'mean',\n",
    "        'm_gps_lat_mean': 'mean',\n",
    "        'm_gps_lon_mean': 'mean'\n",
    "    }).reset_index()\n",
    "\n",
    "    daily_summary['user_id'] = int(user_id)\n",
    "\n",
    "    return daily_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 12/30 [00:05<00:09,  1.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'time'\n",
      "'time'\n",
      "'time'\n",
      "'time'\n",
      "'time'\n",
      "'time'\n",
      "'time'\n",
      "'time'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:09<00:00,  3.12it/s]\n"
     ]
    }
   ],
   "source": [
    "# user01부터 user30까지 반복하면서 각 사용자의 데이터를 처리\n",
    "all_users_summary = pd.DataFrame()\n",
    "\n",
    "for user_id in tqdm(range(1, 31)):\n",
    "    try:\n",
    "        user_summary = process_train_data(user_id, os.path.join(preprocessed_dir_moc, \"train\", \"after_label\"))\n",
    "        all_users_summary = pd.concat([all_users_summary, user_summary], ignore_index=True)\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label = pd.read_csv(os.path.join(data_dir, 'train_label.csv'), index_col=0)\n",
    "train_label[\"user_id\"] = train_label[\"subject_id\"].apply(lambda x: int(x.split(\"r\")[1]))\n",
    "\n",
    "train_label[\"date\"] = pd.to_datetime(train_label[\"date\"]).dt.date\n",
    "all_users_summary[\"date\"] = pd.to_datetime(all_users_summary[\"date\"]).dt.date\n",
    "\n",
    "# train_label과 all_users_summary 병합\n",
    "merged = train_label.merge(all_users_summary, on=['date', 'user_id'], how='right')\n",
    "\n",
    "# 필요없는 컬럼 제거\n",
    "all_users_summary = merged.drop(['Q1', 'Q2', 'Q3', 'S1', 'S2', 'S3', 'S4'], axis=1)\n",
    "all_users_summary.dropna(subset=['subject_id'], inplace=True)\n",
    "all_users_summary.drop(['subject_id'], axis=1, inplace=True)\n",
    "\n",
    "merged.drop(['user_id', 'm_acc_x_mean', 'm_acc_x_var', 'm_acc_y_mean',\n",
    "       'm_acc_y_var', 'm_acc_z_mean', 'm_acc_z_var', 'activity', 's_hr_mean',\n",
    "       'm_gps_lat_mean', 'm_gps_lon_mean'], axis=1, inplace=True)\n",
    "\n",
    "\n",
    "# CSV 파일로 저장\n",
    "all_users_summary.to_csv(os.path.join(preprocessed_dir_moc, 'train_final', 'train_user.csv'), index=False)\n",
    "merged.to_csv(os.path.join(preprocessed_dir_moc, 'train_final', 'train_label.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dir = os.path.join(data_dir, \"val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simplify_column_name_part(column_name):\n",
    "    parts = column_name.split('_')\n",
    "    if len(parts) >= 5 and column_name.startswith(\"m_acc\"):\n",
    "        return '_'.join([parts[0], parts[1], parts[4]])\n",
    "    return column_name\n",
    "\n",
    "def split_sensor_by_user(val_dict, sensor_type, agg_dict=None, interpolate=False, is_data_num=False, preprocess=True, rename_col=None, user_split=True):\n",
    "    if user_split:\n",
    "        grouped = val_dict[sensor_type].groupby('subject_id')\n",
    "    else:\n",
    "        grouped = [(sensor_type[-1], val_dict[sensor_type])]\n",
    "    sensor_type_rename = \"s_\" + sensor_type[2:] if sensor_type.startswith(\"w_\") else sensor_type\n",
    "    for name, group in grouped:\n",
    "        if sensor_type_rename == \"m_activity\":\n",
    "            group[\"m_activity\"] = group[\"m_activity\"].apply(lambda x : int(x))\n",
    "        if sensor_type_rename == \"m_ambience\":\n",
    "            # 각 array 중 array[i][1]이 0.1 이상인 것의 array[i][0]을 return\n",
    "            group[\"ambience_labels\"] = group[\"ambience_labels\"].apply(lambda x : [i for i in x if float(i[1]) >= 0.3])\n",
    "        if sensor_type_rename == \"m_usage_stats\":\n",
    "            group['m_usage_stats'] = group['m_usage_stats'].apply(lambda x: [[i[\"app_name\"], i[\"total_time\"]] for i in x])\n",
    "\n",
    "        del group['subject_id']\n",
    "        group.set_index('timestamp', inplace=True)\n",
    "        if len(group.columns) > 1:\n",
    "            group.columns = [sensor_type_rename + \"_\" + col if col != 'timestamp' else col for col in group.columns]\n",
    "        else:\n",
    "            group.columns = [sensor_type_rename]\n",
    "\n",
    "        if preprocess:\n",
    "            group = group[~group.index.duplicated(keep='first')]\n",
    "            if interpolate:\n",
    "                # resampled = group.resample('1min').interpolate(\"linear\") if is_data_num else group.resample('1min').bfill()\n",
    "                resampled = group.resample('1min').first()\n",
    "\n",
    "            else:\n",
    "                agg_functions = {col: ['mean', 'var'] for col in group.columns} if agg_dict is None else agg_dict\n",
    "                resampled = group.resample('1min').agg(agg_functions)\n",
    "                resampled.columns = rename_col if rename_col is not None else [f'{simplify_column_name_part(i[0])}_{i[1]}' for i in resampled.columns]\n",
    "                \n",
    "        else:\n",
    "            resampled = group\n",
    "        if user_split:\n",
    "            val_dict[f'{sensor_type_rename}_part_{name}'] = resampled\n",
    "        else:\n",
    "            val_dict[sensor_type_rename] = resampled\n",
    "    if user_split:\n",
    "        del grouped, resampled, val_dict[sensor_type]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:16<00:00,  1.41s/it]\n"
     ]
    }
   ],
   "source": [
    "val_files = glob.glob(os.path.join(val_dir, \"*\"))\n",
    "\n",
    "val_dict = {}\n",
    "\n",
    "for val in tqdm(val_files):\n",
    "    extracted_text = (os.path.basename(val).split('__'))[1].split('.')[0]\n",
    "    val_dict[extracted_text] = pd.read_parquet(val)\n",
    "\n",
    "# 전처리 수행\n",
    "## 1초 이하 단위로 기록되어 있는 것들\n",
    "split_sensor_by_user(val_dict, \"m_activity\", {'m_activity': [lambda x: x.mode()[0] if not x.empty else None, \"var\"]}, rename_col=[\"m_activity_mode\", \"m_activity_var\"])\n",
    "split_sensor_by_user(val_dict, \"w_light\", rename_col=[\"s_light_mean\", \"s_light_var\"])\n",
    "split_sensor_by_user(val_dict, \"w_pedo\", None, preprocess=False)\n",
    "for k in val_dict.keys():\n",
    "    if k.startswith(\"m_acc\"):\n",
    "        split_sensor_by_user(val_dict, k, user_split=False)\n",
    "\n",
    "## 1초보다 큰 단위로 기록되어 있는 것들\n",
    "split_sensor_by_user(val_dict, \"w_heart_rate\", None, interpolate=True, is_data_num=True)\n",
    "split_sensor_by_user(val_dict, \"m_light\", None, interpolate=True, is_data_num=True)\n",
    "split_sensor_by_user(val_dict, \"m_usage_stats\", None, interpolate=True, is_data_num=False)\n",
    "split_sensor_by_user(val_dict, \"m_ambience\", None, interpolate=True, is_data_num=False)\n",
    "split_sensor_by_user(val_dict, \"m_gps\", None, interpolate=True, is_data_num=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_parts = {}\n",
    "\n",
    "for i in range(1, 5):\n",
    "    part_data = [df for key, df in val_dict.items() if key.endswith(f\"part_{i}\")]\n",
    "    for idx, part in enumerate(part_data):\n",
    "        if idx == 0:\n",
    "            result_df = part\n",
    "            continue\n",
    "        result_df = pd.merge(result_df, part, how='outer', left_index=True, right_index=True)\n",
    "    merged_parts[f\"part_{i}\"] = result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for part in merged_parts.keys():\n",
    "    daily_groups = merged_parts[part].resample('1D')\n",
    "\n",
    "    for date, group in daily_groups:\n",
    "        user_number = \"{:02}\".format(int(part.split(\"_\")[-1]))\n",
    "        file_name = f'user_{user_number}_{date.strftime(\"%Y%m%d\")}.csv'\n",
    "        os.makedirs(os.path.join(preprocessed_dir_moc, \"val\"), exist_ok=True)\n",
    "        group.to_csv(os.path.join(preprocessed_dir_moc, \"val\", file_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for user in range(1, 5):\n",
    "    user_df = pd.DataFrame()\n",
    "    for user_data in glob.glob(os.path.join(preprocessed_dir_moc, \"val\", f\"user_{user:02d}_*\")):\n",
    "        user_partial = pd.read_csv(user_data)\n",
    "        user_df = user_partial if user_df.empty else pd.concat([user_df, user_partial])\n",
    "    os.makedirs(os.path.join(preprocessed_dir_moc, \"val_after\"), exist_ok=True)\n",
    "    \n",
    "    user_df.to_csv(os.path.join(preprocessed_dir_moc, \"val_after\", f'val_{user:02d}.csv'), index=False)\n",
    "    user_df = pd.read_csv(os.path.join(preprocessed_dir_moc, \"val_after\", f'val_{user:02d}.csv'))\n",
    "\n",
    "    user_df['timestamp'] = pd.to_datetime(user_df['timestamp'])\n",
    "\n",
    "    user_df['date'] = user_df['timestamp'].dt.date\n",
    "\n",
    "    daily_summary = user_df.groupby('date').agg({\n",
    "        'm_acc_x_mean': 'mean',\n",
    "        'm_acc_x_var': 'mean',\n",
    "        'm_acc_y_mean': 'mean',\n",
    "        'm_acc_y_var': 'mean',\n",
    "        'm_acc_z_mean': 'mean',\n",
    "        'm_acc_z_var': 'mean',\n",
    "        'm_activity_mode': 'mean',\n",
    "        'm_activity_var': 'mean',\n",
    "        's_light_mean': 'mean',\n",
    "        's_pedo_step_frequency': 'sum',\n",
    "        's_pedo_walking_steps': 'sum',\n",
    "        's_heart_rate': 'mean',\n",
    "        'm_light': 'mean',\n",
    "        'm_gps_altitude': 'mean',\n",
    "        'm_gps_latitude': 'mean',\n",
    "        'm_gps_longitude': 'mean',\n",
    "        'm_gps_speed': 'mean'\n",
    "    }).reset_index()\n",
    "\n",
    "    os.makedirs(os.path.join(preprocessed_dir_moc, \"val_final\"), exist_ok=True)\n",
    "    daily_summary.to_csv(os.path.join(preprocessed_dir_moc, \"val_final\", f'val_{user:02d}.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data file\n",
    "val_label_df = pd.read_csv(os.path.join(data_dir, \"val_label.csv\"))\n",
    "\n",
    "# Group by 'subject_id' and save each group to a separate CSV file without the 'subject_id' column\n",
    "for subject_id, group_df in val_label_df.groupby('subject_id'):\n",
    "    group_df.drop(columns=['subject_id']).to_csv(os.path.join(preprocessed_dir_moc, \"val_final\", f'val_{subject_id:02d}_label.csv'), index=False)\n",
    "\n",
    "for user in range(1, 5):\n",
    "    val_user = pd.read_csv(os.path.join(preprocessed_dir_moc, \"val_final\", f'val_{user:02d}.csv'))\n",
    "    val_label = pd.read_csv(os.path.join(preprocessed_dir_moc, \"val_final\", f'val_{user:02d}_label.csv'))\n",
    "\n",
    "    merged_df = pd.merge(val_user, val_label, on='date')\n",
    "\n",
    "    merged_df.to_csv(os.path.join(preprocessed_dir_moc, \"val_final\", f'val_{user:02d}_user.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dir = os.path.join(data_dir, \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:18<00:00,  1.54s/it]\n"
     ]
    }
   ],
   "source": [
    "test_files = glob.glob(os.path.join(test_dir, \"*\"))\n",
    "\n",
    "test_dict = {}\n",
    "\n",
    "for test in tqdm(test_files):\n",
    "    extracted_text = (os.path.basename(test).split('__'))[1].split('.')[0]\n",
    "    test_dict[extracted_text] = pd.read_parquet(test)\n",
    "\n",
    "# 데이터 전처리\n",
    "## 1초 이하 단위로 기록되어 있는 것들\n",
    "split_sensor_by_user(test_dict, \"m_activity\", {'m_activity': [lambda x: x.mode()[0] if not x.empty else None, \"var\"]}, rename_col=[\"m_activity_mode\", \"m_activity_var\"])\n",
    "split_sensor_by_user(test_dict, \"w_light\")\n",
    "split_sensor_by_user(test_dict, \"w_pedo\", None, preprocess=False)\n",
    "for k in test_dict.keys():\n",
    "    if k.startswith(\"m_acc\"):\n",
    "        split_sensor_by_user(test_dict, k, user_split=False)\n",
    "\n",
    "## 1초보다 큰 단위로 기록되어 있는 것들\n",
    "split_sensor_by_user(test_dict, \"w_heart_rate\", None, interpolate=True, is_data_num=True)\n",
    "split_sensor_by_user(test_dict, \"m_light\", None, interpolate=True, is_data_num=True)\n",
    "split_sensor_by_user(test_dict, \"m_usage_stats\", None, interpolate=True, is_data_num=False)\n",
    "split_sensor_by_user(test_dict, \"m_ambience\", None, interpolate=True, is_data_num=False)\n",
    "split_sensor_by_user(test_dict, \"m_gps\", None, interpolate=True, is_data_num=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_parts = {}\n",
    "\n",
    "for i in range(5, 9):\n",
    "    part_data = [df for key, df in test_dict.items() if key.endswith(f\"part_{i}\")]\n",
    "    result_df = part_data[0]\n",
    "\n",
    "    for part in part_data[1:]:\n",
    "        result_df = pd.merge(result_df, part, how='outer', left_index=True, right_index=True)\n",
    "    merged_parts[f\"part_{i}\"] = result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for part in merged_parts.keys():\n",
    "    daily_groups = merged_parts[part].resample('1D')\n",
    "\n",
    "    for date, group in daily_groups:\n",
    "        user_number = \"{:02}\".format(int(part.split(\"_\")[-1]))\n",
    "        file_name = f'user_{user_number}_{date.strftime(\"%Y%m%d\")}.csv'\n",
    "        os.makedirs(os.path.join(preprocessed_dir_moc, \"test\"), exist_ok=True)\n",
    "        group.to_csv(os.path.join(preprocessed_dir_moc, \"test\", file_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for user in range(5, 9):\n",
    "    user_df = pd.DataFrame()\n",
    "    for user_data in glob.glob(os.path.join(preprocessed_dir_moc, \"test\", f\"user_{user:02d}_*\")):\n",
    "        user_partial = pd.read_csv(user_data)\n",
    "        user_df = user_partial if user_df.empty else pd.concat([user_df, user_partial])\n",
    "    os.makedirs(os.path.join(preprocessed_dir_moc, \"test_after\"), exist_ok=True)\n",
    "    \n",
    "    user_df.to_csv(os.path.join(preprocessed_dir_moc, \"test_after\", f'test_{user:02d}.csv'), index=False)\n",
    "    user_df = pd.read_csv(os.path.join(preprocessed_dir_moc, \"test_after\", f'test_{user:02d}.csv'))\n",
    "\n",
    "    user_df['timestamp'] = pd.to_datetime(user_df['timestamp'])\n",
    "\n",
    "    user_df['date'] = user_df['timestamp'].dt.date\n",
    "\n",
    "    daily_summary = user_df.groupby('date').agg({\n",
    "        'm_acc_x_mean': 'mean',\n",
    "        'm_acc_x_var': 'mean',\n",
    "        'm_acc_y_mean': 'mean',\n",
    "        'm_acc_y_var': 'mean',\n",
    "        'm_acc_z_mean': 'mean',\n",
    "        'm_acc_z_var': 'mean',\n",
    "        'm_activity_mode': 'mean',\n",
    "        'm_activity_var': 'mean',\n",
    "        's_light_mean': 'mean',\n",
    "        's_pedo_step_frequency': 'sum',\n",
    "        's_pedo_walking_steps': 'sum',\n",
    "        's_heart_rate': 'mean',\n",
    "        'm_light': 'mean',\n",
    "        'm_gps_altitude': 'mean',\n",
    "        'm_gps_latitude': 'mean',\n",
    "        'm_gps_longitude': 'mean',\n",
    "        'm_gps_speed': 'mean'\n",
    "    }).reset_index()\n",
    "\n",
    "    os.makedirs(os.path.join(preprocessed_dir_moc, \"test_final\"), exist_ok=True)\n",
    "    daily_summary.to_csv(os.path.join(preprocessed_dir_moc, \"test_final\", f'test_{user:02d}_user.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "최종 결과물에서 다음 파일을 활용하였습니다.\n",
    "\n",
    "`../data_preprocessed_moc/train_final/train_label.csv`\n",
    "`../data_preprocessed_moc/train_final/train_user.csv`\n",
    "\n",
    "`../data_preprocessed_moc/val_final/val_01_user.csv`\n",
    "`../data_preprocessed_moc/val_final/val_02_user.csv`\n",
    "`../data_preprocessed_moc/val_final/val_03_user.csv`\n",
    "`../data_preprocessed_moc/val_final/val_04_user.csv`\n",
    "\n",
    "`../data_preprocessed_moc/test_final/test_05_user.csv`\n",
    "`../data_preprocessed_moc/test_final/test_06_user.csv`\n",
    "`../data_preprocessed_moc/test_final/test_07_user.csv`\n",
    "`../data_preprocessed_moc/test_final/test_08_user.csv`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ETRI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
