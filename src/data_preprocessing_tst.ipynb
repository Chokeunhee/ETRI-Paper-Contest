{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing of Time Series Transformer\n",
    "\n",
    "Q1-Q3 Column Prediction model \"TST\" 전용 전처리 코드입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import glob\n",
    "import os\n",
    "import gc\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "코드 실행 전에 다음 파일들이 `./data` 경로에 포함되어야 합니다.\n",
    "\n",
    "```\n",
    ".\n",
    "├── [4.0K]  answer_sample.csv\n",
    "├── [9.5K]  README_2020.txt\n",
    "├── [4.0K]  test\n",
    "│   ├── [627M]  ch2024_test__m_acc_part_5.parquet.gzip\n",
    "│   ├── [1.1G]  ch2024_test__m_acc_part_6.parquet.gzip\n",
    "│   ├── [1.0G]  ch2024_test__m_acc_part_7.parquet.gzip\n",
    "│   ├── [935M]  ch2024_test__m_acc_part_8.parquet.gzip\n",
    "│   ├── [912K]  ch2024_test__m_activity.parquet.gzip\n",
    "│   ├── [3.2M]  ch2024_test__m_ambience.parquet.gzip\n",
    "│   ├── [ 12M]  ch2024_test__m_gps.parquet.gzip\n",
    "│   ├── [ 93K]  ch2024_test__m_light.parquet.gzip\n",
    "│   ├── [203K]  ch2024_test__m_usage_stats.parquet.gzip\n",
    "│   ├── [935K]  ch2024_test__w_heart_rate.parquet.gzip\n",
    "│   ├── [106K]  ch2024_test__w_light.parquet.gzip\n",
    "│   └── [909K]  ch2024_test__w_pedo.parquet.gzip\n",
    "├── [4.0K]  train\n",
    "│   ├── [4.0K]  user01\n",
    "│   ├── [4.0K]  user02\n",
    "│   ├── [4.0K]  user03\n",
    "│   ├── [4.0K]  user04\n",
    "│   ├── [4.0K]  user05\n",
    "│   ├── [4.0K]  user06\n",
    "│   ├── [4.0K]  user07\n",
    "│   ├── [4.0K]  user08\n",
    "│   ├── [4.0K]  user09\n",
    "│   ├── [4.0K]  user10\n",
    "│   ├── [4.0K]  user11\n",
    "│   ├── [4.0K]  user12\n",
    "│   ├── [4.0K]  user21\n",
    "│   ├── [4.0K]  user22\n",
    "│   ├── [4.0K]  user23\n",
    "│   ├── [4.0K]  user24\n",
    "│   ├── [4.0K]  user25\n",
    "│   ├── [4.0K]  user26\n",
    "│   ├── [4.0K]  user27\n",
    "│   ├── [4.0K]  user28\n",
    "│   ├── [4.0K]  user29\n",
    "│   ├── [4.0K]  user30\n",
    "│   ├── [1.1K]  user_info_2020.csv\n",
    "│   ├── [ 73K]  user_sleep_2020.csv\n",
    "│   └── [ 89K]  user_survey_2020.csv\n",
    "├── [ 18K]  train_label.csv\n",
    "├── [4.0K]  val\n",
    "│   ├── [1.3G]  ch2024_val__m_acc_part_1.parquet.gzip\n",
    "│   ├── [563M]  ch2024_val__m_acc_part_2.parquet.gzip\n",
    "│   ├── [662M]  ch2024_val__m_acc_part_3.parquet.gzip\n",
    "│   ├── [838M]  ch2024_val__m_acc_part_4.parquet.gzip\n",
    "│   ├── [853K]  ch2024_val__m_activity.parquet.gzip\n",
    "│   ├── [3.7M]  ch2024_val__m_ambience.parquet.gzip\n",
    "│   ├── [ 15M]  ch2024_val__m_gps.parquet.gzip\n",
    "│   ├── [ 88K]  ch2024_val__m_light.parquet.gzip\n",
    "│   ├── [192K]  ch2024_val__m_usage_stats.parquet.gzip\n",
    "│   ├── [925K]  ch2024_val__w_heart_rate.parquet.gzip\n",
    "│   ├── [101K]  ch2024_val__w_light.parquet.gzip\n",
    "│   └── [901K]  ch2024_val__w_pedo.parquet.gzip\n",
    "└── [2.9K]  val_label.csv\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 경로 설정\n",
    "data_dir = \"../data\"  # 데이터 저장 경로\n",
    "preprocessed_dir_ts = \"../data_preprocessed_ts\"  # 전처리 완료된 데이터를 저장할 경로\n",
    "\n",
    "train_label_dir = os.path.join(preprocessed_dir_ts, \"train_label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Regression Label 전처리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### user_survey_2020.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "survey_df = pd.read_csv(os.path.join(data_dir, \"train\", \"user_survey_2020.csv\"))\n",
    "survey_df.sort_values(by=['userId', 'date'], inplace=True)\n",
    "survey_df['date'] = pd.to_datetime(survey_df['date'])\n",
    "survey_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "if pd.isna(survey_df.at[1, 'sleep']):\n",
    "    results = []\n",
    "\n",
    "    for index, row in tqdm(survey_df.iterrows(), total=survey_df.shape[0]):\n",
    "        if row['amPm'] == 'pm' and index + 1 < len(survey_df) and survey_df.iloc[index + 1]['amPm'] == 'am':\n",
    "            next_row = survey_df.iloc[index + 1]\n",
    "            new_row = row.copy()\n",
    "            new_row['sleep'] = next_row['sleep']\n",
    "            new_row['sleepProblem'] = next_row['sleepProblem']\n",
    "            new_row['dream'] = next_row['dream']\n",
    "            new_row['amCondition'] = next_row['amCondition']\n",
    "            new_row['amEmotion'] = next_row['amEmotion']\n",
    "            \n",
    "            results.append(new_row)\n",
    "            \n",
    "    survey_combined_df = pd.DataFrame(results)\n",
    "    survey_combined_df.drop(columns=['amPm', 'startInput', 'endInput'], inplace=True)\n",
    "    survey_combined_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "os.makedirs(train_label_dir, exist_ok=True)\n",
    "survey_combined_df.to_csv(os.path.join(train_label_dir, \"user_survey_2020.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### user_sleep_2020.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_duplicate_rows(df):\n",
    "    # 'userId'와 'date' 기준으로 중복된 데이터를 찾습니다.\n",
    "    duplicates = df.duplicated(subset=['userId', 'date'], keep=False)\n",
    "    # 중복된 행만 필터링합니다.\n",
    "    duplicate_rows = df[duplicates]\n",
    "    # 필터링된 데이터의 개수를 반환합니다.\n",
    "    return len(duplicate_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sleep_df = pd.read_csv(os.path.join(data_dir, \"train\", \"user_sleep_2020.csv\"))\n",
    "sleep_df.sort_values(by=['userId', 'startDt'], inplace=True)\n",
    "sleep_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "sleep_df['date'] = pd.to_datetime(sleep_df['date'])\n",
    "sleep_df['lastUpdate'] = sleep_df['lastUpdate'].apply(lambda x: datetime.datetime.fromtimestamp(x, datetime.timezone(datetime.timedelta(hours=9))))\n",
    "sleep_df['startDt'] = sleep_df['startDt'].apply(lambda x: datetime.datetime.fromtimestamp(x, datetime.timezone(datetime.timedelta(hours=9))))\n",
    "sleep_df['endDt'] = sleep_df['endDt'].apply(lambda x: datetime.datetime.fromtimestamp(x, datetime.timezone(datetime.timedelta(hours=9))))\n",
    "sleep_df.drop(columns=['timezone'], inplace=True)\n",
    "\n",
    "sleep_df[\"time_in_bed\"] = (sleep_df[\"endDt\"] - sleep_df[\"startDt\"]).dt.total_seconds()\n",
    "\n",
    "aggregations = {\n",
    "    'startDt': 'first',\n",
    "    'endDt': 'last',\n",
    "    'lastUpdate': 'last',\n",
    "    'wakeupduration': 'sum',\n",
    "    'lightsleepduration': 'sum',\n",
    "    'deepsleepduration': 'sum',\n",
    "    'wakeupcount': 'sum',\n",
    "    'durationtosleep': 'sum',\n",
    "    'remsleepduration': 'sum',\n",
    "    'durationtowakeup': 'sum',\n",
    "    'hr_average': 'mean',\n",
    "    'hr_min': 'min',\n",
    "    'hr_max': 'max',\n",
    "    'rr_average': 'mean',\n",
    "    'rr_min': 'min',\n",
    "    'rr_max': 'max',\n",
    "    'breathing_disturbances_intensity': 'sum',\n",
    "    'snoring': 'sum',\n",
    "    'snoringepisodecount': 'sum',\n",
    "    'sleep_score': 'mean',\n",
    "    'time_in_bed': 'sum'\n",
    "}\n",
    "\n",
    "sleep_df_agg = sleep_df.groupby(['userId', 'date']).agg(aggregations).reset_index()\n",
    "sleep_df_agg['date'] = sleep_df_agg['date'] - pd.Timedelta(days=1)\n",
    "sleep_df_agg.to_csv(os.path.join(train_label_dir, \"user_sleep_2020.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train_label.csv 와 결합"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label_df = pd.read_csv(os.path.join(data_dir, \"train_label.csv\"), index_col=0)\n",
    "\n",
    "train_label_df['date'] = pd.to_datetime(train_label_df['date'])\n",
    "train_label_df.rename(columns={\"subject_id\": \"userId\"}, inplace=True)\n",
    "\n",
    "result_df = pd.merge(survey_combined_df, sleep_df_agg, on=['userId', 'date'], how='inner')\n",
    "result_df = pd.merge(result_df, train_label_df, on=['userId', 'date'], how='inner')\n",
    "\n",
    "survey_combined_df[\"date\"] = pd.to_datetime(survey_combined_df[\"date\"])\n",
    "\n",
    "result_df[\"y1\"] = result_df[\"sleep\"]\n",
    "result_df[\"y2\"] = result_df[\"pmEmotion\"]\n",
    "result_df[\"y3\"] = result_df[\"pmStress\"]\n",
    "\n",
    "result_df[\"y4\"] = result_df[\"deepsleepduration\"] + result_df[\"lightsleepduration\"] + result_df[\"remsleepduration\"]\n",
    "result_df[\"y5\"] = result_df[\"wakeupduration\"]\n",
    "result_df[\"y6\"] = result_df[\"durationtosleep\"]\n",
    "result_df[\"y7\"] = result_df[\"durationtowakeup\"]\n",
    "\n",
    "result_df.to_csv(os.path.join(train_label_dir, \"y_concated_regression.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_preprocessing_tst import preprocess_train_first, preprocess_train_second_add_activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 해당 작업은 매우 오랜 시간이 소모됩니다. (약 10시간)\n",
    "preprocess_train_first(os.path.join(data_dir, \"train\"), os.path.join(preprocessed_dir_ts, \"train\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_train_second_add_activity(preprocessed_dir_ts, os.path.join(preprocessed_dir_ts, \"train_after\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_csv = pd.read_csv(os.path.join(train_label_dir, \"y_concated_regression.csv\"))\n",
    "label_csv['date'] = pd.to_datetime(label_csv['date'])\n",
    "\n",
    "train_all_df = None\n",
    "y_all_df = None\n",
    "\n",
    "train_max_idx = -1\n",
    "y_max_idx = -1\n",
    "\n",
    "train_check_all, y_check_all = 0, 0\n",
    "\n",
    "for user in tqdm(range(1, 31)):\n",
    "    try:\n",
    "        user_df = pd.read_pickle(os.path.join(preprocessed_dir_ts, \"train_after\", f\"user{user:02d}.pkl\"))\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        continue\n",
    "\n",
    "    user_df['date'] = user_df.index.date\n",
    "    user_df.reset_index(inplace=True)\n",
    "\n",
    "    label_csv_user = label_csv[label_csv['userId'] == f'user{user:02d}']\n",
    "    label_csv_user = label_csv_user[label_csv_user['date'].isin(user_df['date'])]\n",
    "    label_csv_user = label_csv_user[['date', 'Q1', 'Q2', 'Q3', 'S1', 'S2', 'S3', 'S4', 'y1', 'y2', 'y3', 'y4', 'y5', 'y6', 'y7']]\n",
    "\n",
    "    unique_dates = label_csv_user['date'].dt.date.unique()\n",
    "\n",
    "    user_df = user_df[user_df['date'].isin(unique_dates)]\n",
    "    user_df[\"id\"] = user_df['date'].factorize()[0]\n",
    "    user_df[\"id\"] += train_max_idx + 1\n",
    "    user_df.set_index('id', inplace=True)\n",
    "    user_df.drop(columns=['date'], inplace=True)\n",
    "    label_csv_user.reset_index(drop=True, inplace=True)\n",
    "    label_csv_user.index += y_max_idx + 1\n",
    "\n",
    "    train_check_all += len(user_df.index.unique())\n",
    "    y_check_all += len(label_csv_user.index)\n",
    "\n",
    "    train_all_df = user_df if train_all_df is None else pd.concat([train_all_df, user_df])\n",
    "    train_max_idx = train_all_df.index.max()\n",
    "    y_all_df = label_csv_user if y_all_df is None else pd.concat([y_all_df, label_csv_user])\n",
    "    y_max_idx = y_all_df.index.max()\n",
    "\n",
    "train_all_df.drop(columns=['time'], inplace=True)\n",
    "y_all_df.drop(columns=['date'], inplace=True)\n",
    "\n",
    "print(f\"train_check_all: {train_check_all}, y_check_all: {y_check_all}\")\n",
    "print(\"train_all_df.shape\", len(train_all_df.index.unique()), \"y_all_df.shape\", y_all_df.shape[0])\n",
    "\n",
    "os.makedirs(os.path.join(preprocessed_dir_ts, \"train_final\"), exist_ok=True)\n",
    "train_all_df.to_csv(os.path.join(preprocessed_dir_ts, \"train_final\", \"train_all.csv\"))\n",
    "y_all_df.to_csv(os.path.join(preprocessed_dir_ts, \"train_final\", \"train_y_all.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_sensor_by_user(dic, sensor_type):\n",
    "    grouped = dic[sensor_type].groupby('subject_id')\n",
    "    sensor_type_rename = \"s_\" + sensor_type[2:] if sensor_type.startswith(\"w_\") else sensor_type\n",
    "    for name, group in tqdm(grouped):\n",
    "        if sensor_type_rename == \"m_activity\":\n",
    "            group[\"m_activity\"] = group[\"m_activity\"].apply(lambda x : int(x))\n",
    "            group_one_hot_label = pd.get_dummies(group[\"m_activity\"], prefix='m_activity')\n",
    "            group = pd.concat([group, group_one_hot_label], axis=1)\n",
    "            group.set_index('timestamp', inplace=True)\n",
    "            group = group.resample('1S').ffill().bfill()\n",
    "            group.drop(columns=[\"m_activity\"], inplace=True)\n",
    "            for i in range(9):\n",
    "                if f'm_activity_{i}' not in group.columns:\n",
    "                    group[f'm_activity_{i}'] = np.int8(0)\n",
    "                elif f'm_activity_{i}' in group.columns:\n",
    "                    group[f'm_activity_{i}'] = group[f'm_activity_{i}'].astype(np.int8)\n",
    "            group.reset_index(inplace=True)\n",
    "\n",
    "        group.drop(columns=['subject_id'], inplace=True)\n",
    "        group.set_index('timestamp', inplace=True)\n",
    "        group.index.names = ['time']\n",
    "            \n",
    "        if sensor_type_rename != \"m_activity\":\n",
    "            if len(group.columns) > 1:\n",
    "                group.columns = [sensor_type_rename + \"_\" + col if col != 'timestamp' else col for col in group.columns]\n",
    "            else:\n",
    "                group.columns = [sensor_type_rename]\n",
    "\n",
    "        if sensor_type_rename not in [\"m_activity\"]:\n",
    "            group = group[~group.index.duplicated(keep='first')]\n",
    "            resampled = group.resample('1S').mean().interpolate(method='time')\n",
    "        else:\n",
    "            resampled = group\n",
    "        dic[f'{sensor_type_rename}_{name}'] = resampled\n",
    "    print(f\"Deleting {sensor_type}\")\n",
    "    del grouped, resampled, dic[sensor_type]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_val_test(is_val=True):\n",
    "    user_list = list(range(1, 5)) if is_val else list(range(5, 9))\n",
    "    file_path = glob.glob(os.path.join(data_dir, \"val\", \"*\")) if is_val else glob.glob(os.path.join(data_dir, \"test\", \"*\"))\n",
    "    dic = {}\n",
    "\n",
    "    for val in tqdm(file_path, desc=\"Reading files to dict\"):\n",
    "        extracted_text = (os.path.basename(val).split('__'))[1].split('.')[0]\n",
    "        dic[extracted_text] = pd.read_parquet(val)\n",
    "\n",
    "    dic[\"m_acc\"] = pd.concat([\n",
    "        dic[f\"m_acc_part_{user_list[0]}\"].reset_index(drop=True),\n",
    "        dic[f\"m_acc_part_{user_list[1]}\"].reset_index(drop=True),\n",
    "        dic[f\"m_acc_part_{user_list[2]}\"].reset_index(drop=True),\n",
    "        dic[f\"m_acc_part_{user_list[3]}\"].reset_index(drop=True)\n",
    "    ], ignore_index=True)\n",
    "\n",
    "    del dic[f\"m_acc_part_{user_list[0]}\"], dic[f\"m_acc_part_{user_list[1]}\"], dic[f\"m_acc_part_{user_list[2]}\"], dic[f\"m_acc_part_{user_list[3]}\"]\n",
    "    del dic[\"m_light\"], dic[\"m_usage_stats\"], dic[\"w_pedo\"], dic[\"w_light\"], dic[\"m_ambience\"]\n",
    "\n",
    "    gc.collect()\n",
    "\n",
    "    split_sensor_by_user(dic, \"m_acc\")\n",
    "    split_sensor_by_user(dic, \"m_gps\")\n",
    "    split_sensor_by_user(dic, \"m_activity\")\n",
    "    split_sensor_by_user(dic, \"w_heart_rate\")\n",
    "\n",
    "    return dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading files to dict: 100%|██████████| 12/12 [00:17<00:00,  1.42s/it]\n",
      "100%|██████████| 4/4 [03:31<00:00, 52.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting m_acc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:04<00:00,  1.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting m_gps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:03<00:00,  1.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting m_activity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:01<00:00,  2.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting w_heart_rate\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "val_dict = preprocess_val_test(is_val=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:15<00:00,  3.98s/it]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(1, 5)):\n",
    "    os.makedirs(os.path.join(preprocessed_dir_ts, \"val\"), exist_ok=True)\n",
    "    user_df = None\n",
    "    for key, value in val_dict.items():\n",
    "        if key.endswith(f\"_{i}\"):\n",
    "            user_df = value if user_df is None else pd.concat([user_df, value], join='outer', axis=1)\n",
    "    for col in user_df.columns:\n",
    "        if not col.startswith(\"m_activity\"):\n",
    "            user_df[col] = user_df[col].interpolate(method='time')\n",
    "        else:\n",
    "            user_df[col] = user_df[col].ffill().bfill()\n",
    "    user_df.to_pickle(os.path.join(preprocessed_dir_ts, \"val\", f\"user{i:02d}.pkl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for user in tqdm(range(1, 5)):\n",
    "    user_df = pd.read_pickle(os.path.join(preprocessed_dir_ts, \"val\", f\"user{user:02d}.pkl\"))\n",
    "\n",
    "    # m_activity 컬럼이 없는지 확인하고 없으면 추가\n",
    "    for i in range(9):\n",
    "        if f'm_activity_{i}' not in user_df.columns:\n",
    "            user_df[f'm_activity_{i}'] = None\n",
    "    \n",
    "    user_df.drop(columns=['m_gps_altitude', 'm_gps_speed'], inplace=True, errors='ignore')\n",
    "\n",
    "    user_df = user_df[['m_acc_x', 'm_acc_y', 'm_acc_z',\n",
    "                       'm_gps_latitude', 'm_gps_longitude', 's_heart_rate',\n",
    "                       'm_activity_0', 'm_activity_1', 'm_activity_2', 'm_activity_3',\n",
    "                       'm_activity_4', 'm_activity_5', 'm_activity_6', 'm_activity_7', 'm_activity_8']]\n",
    "    \n",
    "    resample_dict_10_min = {\n",
    "        'm_acc_x': ['mean', 'std', 'min', 'max'],\n",
    "        'm_acc_y': ['mean', 'std', 'min', 'max'],\n",
    "        'm_acc_z': ['mean', 'std', 'min', 'max'],\n",
    "        'm_gps_latitude': ['mean', 'std', 'min', 'max'],\n",
    "        'm_gps_longitude': ['mean', 'std', 'min', 'max'],\n",
    "        's_heart_rate': ['mean', 'std', 'min', 'max'],\n",
    "        'm_activity_0': ['max'],\n",
    "        'm_activity_1': ['max'],\n",
    "        'm_activity_2': ['max'],\n",
    "        'm_activity_3': ['max'],\n",
    "        'm_activity_4': ['max'],\n",
    "        'm_activity_5': ['max'],\n",
    "        'm_activity_6': ['max'],\n",
    "        'm_activity_7': ['max'],\n",
    "        'm_activity_8': ['max']\n",
    "    }\n",
    "    # resample\n",
    "    user_df = user_df.resample('10min').agg(resample_dict_10_min)\n",
    "    # 컬럼명 수정\n",
    "    user_df.columns = ['_'.join(col).strip() if isinstance(col, tuple) else col for col in user_df.columns]\n",
    "    user_df.ffill(inplace=True)\n",
    "    user_df.bfill(inplace=True)\n",
    "    os.makedirs(os.path.join(preprocessed_dir_ts, \"val_after\"), exist_ok=True)\n",
    "\n",
    "    user_df.to_pickle(os.path.join(preprocessed_dir_ts, \"val_after\", f\"user{user:02d}.pkl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_csv = pd.read_csv(os.path.join(data_dir, \"val_label.csv\"))\n",
    "label_csv['date'] = pd.to_datetime(label_csv['date'])\n",
    "\n",
    "val_test_all_df = None\n",
    "y_all_df = None\n",
    "\n",
    "val_test_max_idx = -1\n",
    "y_max_idx = -1\n",
    "\n",
    "val_test_check_all, y_check_all = 0, 0\n",
    "\n",
    "for user in tqdm(range(1, 5)):\n",
    "    try:\n",
    "        user_df = pd.read_pickle(os.path.join(preprocessed_dir_ts, \"val_after\", f\"user{user:02d}.pkl\"))\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        continue\n",
    "\n",
    "    user_df['date'] = user_df.index.date\n",
    "    user_df.reset_index(inplace=True)\n",
    "\n",
    "    label_csv_user = label_csv[label_csv['subject_id'] == user]\n",
    "    label_csv_user = label_csv_user[label_csv_user['date'].isin(user_df['date'])]\n",
    "    label_csv_user = label_csv_user[['date', 'Q1', 'Q2', 'Q3', 'S1', 'S2', 'S3', 'S4']]\n",
    "\n",
    "    unique_dates = label_csv_user['date'].dt.date.unique()\n",
    "\n",
    "    user_df = user_df[user_df['date'].isin(unique_dates)]\n",
    "    user_df[\"id\"] = user_df['date'].factorize()[0]\n",
    "    user_df[\"id\"] += val_test_max_idx + 1\n",
    "    user_df.set_index('id', inplace=True)\n",
    "    user_df.drop(columns=['date'], inplace=True)\n",
    "    label_csv_user.reset_index(drop=True, inplace=True)\n",
    "    label_csv_user.index += y_max_idx + 1\n",
    "\n",
    "    val_test_check_all += len(user_df.index.unique())\n",
    "    y_check_all += len(label_csv_user.index)\n",
    "    \n",
    "    val_test_all_df = user_df if val_test_all_df is None else pd.concat([val_test_all_df, user_df])\n",
    "    val_test_max_idx = val_test_all_df.index.max()\n",
    "    y_all_df = label_csv_user if y_all_df is None else pd.concat([y_all_df, label_csv_user])\n",
    "    y_max_idx = y_all_df.index.max()\n",
    "\n",
    "val_test_all_df.drop(columns=['time'], inplace=True)\n",
    "y_all_df.drop(columns=['date'], inplace=True)\n",
    "\n",
    "print(val_test_all_df.columns)\n",
    "print(y_all_df.columns)\n",
    "print(f\"val_test_check_all: {val_test_check_all}, y_check_all: {y_check_all}\")\n",
    "print(\"val_test_all_df.shape\", len(val_test_all_df.index.unique()), \"y_all_df.shape\", y_all_df.shape[0])\n",
    "\n",
    "os.makedirs(os.path.join(preprocessed_dir_ts, \"val_final\"), exist_ok=True)\n",
    "val_test_all_df.to_csv(os.path.join(preprocessed_dir_ts, \"val_final\", \"val_all.csv\"))\n",
    "y_all_df.to_csv(os.path.join(preprocessed_dir_ts, \"val_final\", \"val_y_all.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TST Pretraining을 위해서, Train과 Validation 합하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(os.path.join(preprocessed_dir_ts, \"train_final\", \"train_all.csv\"), index_col=0)\n",
    "val_df = pd.read_csv(os.path.join(preprocessed_dir_ts, \"val_final\", \"val_all.csv\"), index_col=0)\n",
    "val_df.index += (max(list(train_df.index.unique())) + 1)\n",
    "\n",
    "train_y_df = pd.read_csv(os.path.join(preprocessed_dir_ts, \"train_final\", \"train_y_all.csv\"), index_col=0)\n",
    "val_y_df = pd.read_csv(os.path.join(preprocessed_dir_ts, \"val_final\", \"val_y_all.csv\"), index_col=0)\n",
    "val_y_df.index += (max(list(train_y_df.index.unique())) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_concat = pd.concat([train_df, val_df])\n",
    "train_val_y_concat = pd.concat([train_y_df, val_y_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_concat.to_csv(os.path.join(preprocessed_dir_ts, \"train_val_concat.csv\"))\n",
    "train_val_y_concat.to_csv(os.path.join(preprocessed_dir_ts, \"train_val_y_concat.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading files to dict: 100%|██████████| 12/12 [00:49<00:00,  4.12s/it]\n",
      "100%|██████████| 4/4 [03:44<00:00, 56.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting m_acc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:05<00:00,  1.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting m_gps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:03<00:00,  1.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting m_activity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:01<00:00,  2.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting w_heart_rate\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_dict = preprocess_val_test(is_val=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:08<00:00,  2.22s/it]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(5, 9)):\n",
    "    os.makedirs(os.path.join(preprocessed_dir_ts, \"test\"), exist_ok=True)\n",
    "    user_df = None\n",
    "    for key, value in test_dict.items():\n",
    "        if key.endswith(f\"_{i}\"):\n",
    "            user_df = value if user_df is None else pd.concat([user_df, value], join='outer', axis=1)\n",
    "    user_df.to_pickle(os.path.join(preprocessed_dir_ts, \"test\", f\"user{i:02d}.pkl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for user in range(5, 9):\n",
    "    user_df = pd.read_pickle(os.path.join(preprocessed_dir_ts, \"test\", f\"user{user:02d}.pkl\"))\n",
    "\n",
    "    # m_activity 컬럼이 없는지 확인하고 없으면 추가\n",
    "    for i in range(9):\n",
    "        if f'm_activity_{i}' not in user_df.columns:\n",
    "            user_df[f'm_activity_{i}'] = None\n",
    "    \n",
    "    user_df.drop(columns=['m_gps_altitude', 'm_gps_speed'], inplace=True, errors='ignore')\n",
    "\n",
    "    user_df = user_df[['m_acc_x', 'm_acc_y', 'm_acc_z',\n",
    "                       'm_gps_latitude', 'm_gps_longitude', 's_heart_rate',\n",
    "                       'm_activity_0', 'm_activity_1', 'm_activity_2', 'm_activity_3',\n",
    "                       'm_activity_4', 'm_activity_5', 'm_activity_6', 'm_activity_7', 'm_activity_8']]\n",
    "    \n",
    "    resample_dict_10_min = {\n",
    "        'm_acc_x': ['mean', 'std', 'min', 'max'],\n",
    "        'm_acc_y': ['mean', 'std', 'min', 'max'],\n",
    "        'm_acc_z': ['mean', 'std', 'min', 'max'],\n",
    "        'm_gps_latitude': ['mean', 'std', 'min', 'max'],\n",
    "        'm_gps_longitude': ['mean', 'std', 'min', 'max'],\n",
    "        's_heart_rate': ['mean', 'std', 'min', 'max'],\n",
    "        'm_activity_0': ['max'],\n",
    "        'm_activity_1': ['max'],\n",
    "        'm_activity_2': ['max'],\n",
    "        'm_activity_3': ['max'],\n",
    "        'm_activity_4': ['max'],\n",
    "        'm_activity_5': ['max'],\n",
    "        'm_activity_6': ['max'],\n",
    "        'm_activity_7': ['max'],\n",
    "        'm_activity_8': ['max']\n",
    "    }\n",
    "    # resample\n",
    "    user_df = user_df.resample('10min').agg(resample_dict_10_min)\n",
    "    # 컬럼명 수정\n",
    "    user_df.columns = ['_'.join(col).strip() if isinstance(col, tuple) else col for col in user_df.columns]\n",
    "    user_df.ffill(inplace=True)\n",
    "    user_df.bfill(inplace=True)\n",
    "    os.makedirs(os.path.join(preprocessed_dir_ts, \"test_after\"), exist_ok=True)\n",
    "\n",
    "    user_df.to_pickle(os.path.join(preprocessed_dir_ts, \"test_after\", f\"user{user:02d}.pkl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "label_csv = pd.read_csv(os.path.join(data_dir, \"answer_sample.csv\"))\n",
    "label_csv['date'] = pd.to_datetime(label_csv['date'])\n",
    "\n",
    "val_test_all_df = None\n",
    "y_all_df = None\n",
    "\n",
    "val_test_max_idx = -1\n",
    "y_max_idx = -1\n",
    "\n",
    "val_test_check_all, y_check_all = 0, 0\n",
    "\n",
    "for user in tqdm(range(5, 9)):\n",
    "    try:\n",
    "        user_df = pd.read_pickle(os.path.join(preprocessed_dir_ts, \"test_after\", f\"user{user:02d}.pkl\"))\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        continue\n",
    "\n",
    "    user_df['date'] = user_df.index.date\n",
    "    user_df.reset_index(inplace=True)\n",
    "\n",
    "    label_csv_user = label_csv[label_csv['subject_id'] == user]\n",
    "    label_csv_user = label_csv_user[label_csv_user['date'].isin(user_df['date'])]\n",
    "\n",
    "    unique_dates = label_csv_user['date'].dt.date.unique()\n",
    "\n",
    "    user_df = user_df[user_df['date'].isin(unique_dates)]\n",
    "    user_df[\"id\"] = user_df['date'].factorize()[0]\n",
    "    user_df[\"id\"] += val_test_max_idx + 1\n",
    "    user_df.set_index('id', inplace=True)\n",
    "    user_df.drop(columns=['date'], inplace=True)\n",
    "    label_csv_user.reset_index(drop=True, inplace=True)\n",
    "    label_csv_user.index += y_max_idx + 1\n",
    "\n",
    "    val_test_check_all += len(user_df.index.unique())\n",
    "    y_check_all += len(label_csv_user.index)\n",
    "    \n",
    "    val_test_all_df = user_df if val_test_all_df is None else pd.concat([val_test_all_df, user_df])\n",
    "    val_test_max_idx = val_test_all_df.index.max()\n",
    "    y_all_df = label_csv_user if y_all_df is None else pd.concat([y_all_df, label_csv_user])\n",
    "    y_max_idx = y_all_df.index.max()\n",
    "\n",
    "\n",
    "val_test_all_df.drop(columns=['time'], inplace=True)\n",
    "y_all_df.drop(columns=['subject_id', 'date'], inplace=True)\n",
    "\n",
    "print(f\"val_test_check_all: {val_test_check_all}, y_check_all: {y_check_all}\")\n",
    "print(\"val_test_all_df.shape\", len(val_test_all_df.index.unique()), \"y_all_df.shape\", y_all_df.shape[0])\n",
    "\n",
    "os.makedirs(os.path.join(preprocessed_dir_ts, \"test_final\"), exist_ok=True)\n",
    "val_test_all_df.to_csv(os.path.join(preprocessed_dir_ts, \"test_final\", \"test_all.csv\"))\n",
    "y_all_df.to_csv(os.path.join(preprocessed_dir_ts, \"test_final\", \"test_y_all.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "최종 결과물에서 다음 파일을 활용하였습니다.\n",
    "\n",
    "`../data_preprocessed_ts/train_val_concat.csv`  \n",
    "`../data_preprocessed_ts/train_val_y_concat.csv`\n",
    "\n",
    "`../data_preprocessed_ts/train_final/train_all.csv`  \n",
    "`../data_preprocessed_ts/train_final/train_y_all.csv`\n",
    "\n",
    "`../data_preprocessed_ts/test_final/test_all.csv`  \n",
    "`../data_preprocessed_ts/test_final/test_y_all.csv`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ETRI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.-1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
